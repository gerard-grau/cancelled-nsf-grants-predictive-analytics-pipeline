{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a1fd225e",
   "metadata": {},
   "source": [
    "# View Historical MLflow Results\n",
    "\n",
    "Query and visualize results from previous training runs stored in MLflow."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7a4266d0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "====================================================================================================\n",
      "AVAILABLE EXPERIMENTS\n",
      "====================================================================================================\n",
      "Experiment: nsf_models_flagged_words (ID: 352756237987121497)\n",
      "\n",
      "====================================================================================================\n",
      "RUNS FOR EXPERIMENT: nsf_models_flagged_words\n",
      "====================================================================================================\n",
      "\n",
      "                          run_id                       start_time        runName  valid_accuracy  valid_f1_score  valid_auc_roc\n",
      "d6157cc3845e444f8289b105a5691b91 2025-12-15 09:45:30.661000+00:00  lr_tuned_best          0.8952        0.057554       0.702426\n",
      "dc7437361b8d4a5eb1cc6f0d2d97b07a 2025-12-15 09:37:41.265000+00:00 gbt_tuned_best          0.8776        0.072727       0.654807\n",
      "ddec7ab8dc594b28a58b792959ee8bc9 2025-12-15 09:35:15.691000+00:00  rf_tuned_best          0.9344        0.068182       0.673002\n",
      "\n",
      "====================================================================================================\n",
      "Total runs found: 3\n",
      "\n",
      "üèÜ Best F1 Score: 0.0727 (gbt_tuned_best)\n",
      "üèÜ Best Accuracy: 0.9344 (rf_tuned_best)\n"
     ]
    }
   ],
   "source": [
    "# View all historical MLflow experiments and runs\n",
    "import mlflow\n",
    "import pandas as pd\n",
    "\n",
    "mlflow.set_tracking_uri(\"file:./mlruns\")\n",
    "\n",
    "# List all experiments\n",
    "print(\"=\"*100)\n",
    "print(\"AVAILABLE EXPERIMENTS\")\n",
    "print(\"=\"*100)\n",
    "experiments = mlflow.search_experiments()\n",
    "for exp in experiments:\n",
    "    print(f\"Experiment: {exp.name} (ID: {exp.experiment_id})\")\n",
    "\n",
    "# Query specific experiment (change name if needed)\n",
    "experiment_name = \"nsf_models_flagged_words\"  # or \"nsf_models\" for old runs\n",
    "experiment = mlflow.get_experiment_by_name(experiment_name)\n",
    "\n",
    "if experiment:\n",
    "    print(f\"\\n{'='*100}\")\n",
    "    print(f\"RUNS FOR EXPERIMENT: {experiment_name}\")\n",
    "    print(f\"{'='*100}\\n\")\n",
    "    \n",
    "    # Get all runs for this experiment\n",
    "    runs = mlflow.search_runs(experiment_ids=[experiment.experiment_id])\n",
    "    \n",
    "    if len(runs) > 0:\n",
    "        # Display key metrics\n",
    "        display_cols = ['run_id', 'start_time', 'tags.mlflow.runName', \n",
    "                       'metrics.valid_accuracy', 'metrics.valid_f1_score', 'metrics.valid_auc_roc',\n",
    "                       'metrics.test_accuracy', 'metrics.test_f1_score', 'metrics.test_auc_roc']\n",
    "        \n",
    "        available_cols = [col for col in display_cols if col in runs.columns]\n",
    "        \n",
    "        if len(available_cols) > 0:\n",
    "            results_df = runs[available_cols].sort_values('start_time', ascending=False)\n",
    "            \n",
    "            # Rename columns for readability\n",
    "            results_df.columns = [col.replace('metrics.', '').replace('tags.mlflow.', '') for col in results_df.columns]\n",
    "            \n",
    "            print(results_df.to_string(index=False))\n",
    "            print(f\"\\n{'='*100}\")\n",
    "            print(f\"Total runs found: {len(runs)}\")\n",
    "            \n",
    "            # Find best models (only if metrics exist)\n",
    "            if 'metrics.valid_f1_score' in runs.columns and not runs['metrics.valid_f1_score'].isna().all():\n",
    "                best_f1_run = runs.loc[runs['metrics.valid_f1_score'].idxmax()]\n",
    "                print(f\"\\nüèÜ Best F1 Score: {best_f1_run['metrics.valid_f1_score']:.4f} ({best_f1_run.get('tags.mlflow.runName', 'N/A')})\")\n",
    "            \n",
    "            if 'metrics.valid_accuracy' in runs.columns and not runs['metrics.valid_accuracy'].isna().all():\n",
    "                best_acc_run = runs.loc[runs['metrics.valid_accuracy'].idxmax()]\n",
    "                print(f\"üèÜ Best Accuracy: {best_acc_run['metrics.valid_accuracy']:.4f} ({best_acc_run.get('tags.mlflow.runName', 'N/A')})\")\n",
    "        else:\n",
    "            print(\"‚ö†Ô∏è  No metric columns found in runs\")\n",
    "            print(f\"\\nAvailable columns: {list(runs.columns)[:10]}...\")\n",
    "    else:\n",
    "        print(\"‚ö†Ô∏è  No runs found for this experiment\")\n",
    "else:\n",
    "    print(f\"‚ö†Ô∏è  Experiment '{experiment_name}' not found\")\n",
    "    print(\"\\nAvailable experiments:\")\n",
    "    for exp in experiments:\n",
    "        print(f\"  - {exp.name}\")\n",
    "    print(\"\\nChange the experiment_name variable to one of the above and re-run the cell\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80cd6f4f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
