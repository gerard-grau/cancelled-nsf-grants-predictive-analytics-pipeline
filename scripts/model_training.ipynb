{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b1af22bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import mlflow\n",
    "from read_delta_files_utils import load_train, load_test, create_spark_with_delta\n",
    "from model_training_utils import MLExperimentManager"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c50d0875",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Using incubator modules: jdk.incubator.vector\n",
      "Using Spark's default log4j profile: org/apache/spark/log4j2-defaults.properties\n",
      "25/12/12 17:06:19 WARN Utils: Your hostname, G16-Linux, resolves to a loopback address: 127.0.1.1; using 192.168.1.46 instead (on interface wlp0s20f3)\n",
      "25/12/12 17:06:19 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n",
      ":: loading settings :: url = jar:file:/home/gerard/Documents/UPC/41_4t-Q1/BDA/venv-BDA/lib/python3.12/site-packages/pyspark/jars/ivy-2.5.3.jar!/org/apache/ivy/core/settings/ivysettings.xml\n",
      "Ivy Default Cache set to: /home/gerard/.ivy2.5.2/cache\n",
      "The jars for the packages stored in: /home/gerard/.ivy2.5.2/jars\n",
      "io.delta#delta-spark_2.13 added as a dependency\n",
      ":: resolving dependencies :: org.apache.spark#spark-submit-parent-adcaa890-ca8c-4a5b-b571-368bd467a823;1.0\n",
      "\tconfs: [default]\n",
      "\tfound io.delta#delta-spark_2.13;4.0.0 in central\n",
      "\tfound io.delta#delta-storage;4.0.0 in central\n",
      "\tfound org.antlr#antlr4-runtime;4.13.1 in central\n",
      ":: resolution report :: resolve 103ms :: artifacts dl 5ms\n",
      "\t:: modules in use:\n",
      "\tio.delta#delta-spark_2.13;4.0.0 from central in [default]\n",
      "\tio.delta#delta-storage;4.0.0 from central in [default]\n",
      "\torg.antlr#antlr4-runtime;4.13.1 from central in [default]\n",
      "\t---------------------------------------------------------------------\n",
      "\t|                  |            modules            ||   artifacts   |\n",
      "\t|       conf       | number| search|dwnlded|evicted|| number|dwnlded|\n",
      "\t---------------------------------------------------------------------\n",
      "\t|      default     |   3   |   0   |   0   |   0   ||   3   |   0   |\n",
      "\t---------------------------------------------------------------------\n",
      ":: retrieving :: org.apache.spark#spark-submit-parent-adcaa890-ca8c-4a5b-b571-368bd467a823\n",
      "\tconfs: [default]\n",
      "\t0 artifacts copied, 3 already retrieved (0kB/3ms)\n",
      "25/12/12 17:06:20 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "Using Spark's default log4j profile: org/apache/spark/log4j2-defaults.properties\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n"
     ]
    }
   ],
   "source": [
    "spark = create_spark_with_delta()\n",
    "train = load_train(spark)\n",
    "test = load_test(spark)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4b9f950e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/gerard/Documents/UPC/41_4t-Q1/BDA/venv-BDA/lib/python3.12/site-packages/mlflow/tracking/_tracking_service/utils.py:177: FutureWarning: The filesystem tracking backend (e.g., './mlruns') will be deprecated in February 2026. Consider transitioning to a database backend (e.g., 'sqlite:///mlflow.db') to take advantage of the latest MLflow features. See https://github.com/mlflow/mlflow/issues/18534 for more details and migration guidance.\n",
      "  return FileStore(store_uri, store_uri)\n",
      "2025/12/12 17:06:23 INFO mlflow.tracking.fluent: Experiment with name 'nsf_models' does not exist. Creating a new experiment.\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import col, when\n",
    "from pyspark.sql.types import DoubleType, IntegerType, LongType, FloatType, BooleanType\n",
    "import os\n",
    "\n",
    "# Convert boolean target to integer if needed\n",
    "if train.schema[\"is_terminated\"].dataType == BooleanType():\n",
    "    train = train.withColumn(\"is_terminated\", \n",
    "                             when(col(\"is_terminated\") == True, 1).otherwise(0).cast(\"integer\"))\n",
    "    test = test.withColumn(\"is_terminated\", \n",
    "                           when(col(\"is_terminated\") == True, 1).otherwise(0).cast(\"integer\"))\n",
    "\n",
    "# Filter to numeric columns only\n",
    "numeric_cols = [field.name for field in train.schema.fields \n",
    "                if isinstance(field.dataType, (DoubleType, IntegerType, LongType, FloatType))]\n",
    "\n",
    "train = train.dropna(how=\"any\", subset=numeric_cols)\n",
    "test = test.dropna(how=\"any\", subset=numeric_cols)\n",
    "\n",
    "target_col = \"is_terminated\"\n",
    "feature_cols = [c for c in numeric_cols if c != target_col]\n",
    "\n",
    "# Data loaders for manager\n",
    "def load_train_converted(spark):\n",
    "    return train.select(feature_cols + [target_col])\n",
    "\n",
    "def load_test_converted(spark):\n",
    "    return test.select(feature_cols + [target_col])\n",
    "\n",
    "# Initialize manager\n",
    "mlruns_path = os.path.abspath(\"./mlruns\")\n",
    "os.makedirs(mlruns_path, exist_ok=True)\n",
    "\n",
    "manager = MLExperimentManager(\n",
    "    spark=spark,\n",
    "    target_col=target_col,\n",
    "    feature_cols=feature_cols,\n",
    "    experiment_name=\"nsf_models\",\n",
    "    tracking_uri=f\"file:{mlruns_path}\",\n",
    "    problem_type=\"multiclass\",\n",
    ")\n",
    "\n",
    "manager.load_data(load_train_converted, load_test_converted)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c42a5a44",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/12/12 17:06:24 WARN SparkStringUtils: Truncated the string representation of a plan since it was too large. This behavior can be adjusted by setting 'spark.sql.debug.maxToStringFields'.\n",
      "25/12/12 17:06:30 WARN DAGScheduler: Broadcasting large task binary with size 1298.5 KiB\n",
      "25/12/12 17:06:31 WARN DAGScheduler: Broadcasting large task binary with size 1587.3 KiB\n",
      "25/12/12 17:06:31 WARN DAGScheduler: Broadcasting large task binary with size 1839.7 KiB\n",
      "25/12/12 17:06:32 WARN DAGScheduler: Broadcasting large task binary with size 1185.9 KiB\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random Forest - Val: 0.8336, Test: 0.8420\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/12/12 17:06:32 WARN DAGScheduler: Broadcasting large task binary with size 1178.1 KiB\n"
     ]
    }
   ],
   "source": [
    "rf_model, rf_metrics = manager.train_random_forest(\n",
    "    run_name=\"rf_baseline\",\n",
    "    num_trees=100,\n",
    "    max_depth=10,\n",
    ")\n",
    "\n",
    "rf_test_metrics = manager.evaluate_on_test(rf_model)\n",
    "print(f\"Random Forest - Val: {rf_metrics['valid_accuracy']:.4f}, Test: {rf_test_metrics['test_accuracy']:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0b84ee92",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient Boosted Trees - Val: 0.7128, Test: 0.7174\n"
     ]
    }
   ],
   "source": [
    "gbt_model, gbt_metrics = manager.train_gbt(\n",
    "    run_name=\"gbt_baseline\",\n",
    "    max_iter=100,\n",
    "    max_depth=5,\n",
    ")\n",
    "\n",
    "gbt_test_metrics = manager.evaluate_on_test(gbt_model)\n",
    "print(f\"Gradient Boosted Trees - Val: {gbt_metrics['valid_accuracy']:.4f}, Test: {gbt_test_metrics['test_accuracy']:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "51cbefac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logistic Regression - Val: 0.7648, Test: 0.7367\n"
     ]
    }
   ],
   "source": [
    "lr_model, lr_metrics = manager.train_logistic_regression(\n",
    "    run_name=\"lr_baseline\",\n",
    "    reg_param=0.01,\n",
    "    elastic_net_param=0.0,\n",
    ")\n",
    "\n",
    "lr_test_metrics = manager.evaluate_on_test(lr_model)\n",
    "print(f\"Logistic Regression - Val: {lr_metrics['valid_accuracy']:.4f}, Test: {lr_test_metrics['test_accuracy']:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "eccc20d1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "MODEL COMPARISON\n",
      "================================================================================\n",
      "                 Model  Val Accuracy  Test Accuracy\n",
      "         Random Forest        0.8336       0.842004\n",
      "Gradient Boosted Trees        0.7128       0.717405\n",
      "   Logistic Regression        0.7648       0.736673\n",
      "================================================================================\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "comparison_df = pd.DataFrame({\n",
    "    'Model': ['Random Forest', 'Gradient Boosted Trees', 'Logistic Regression'],\n",
    "    'Val Accuracy': [rf_metrics['valid_accuracy'], gbt_metrics['valid_accuracy'], lr_metrics['valid_accuracy']],\n",
    "    'Test Accuracy': [rf_test_metrics['test_accuracy'], gbt_test_metrics['test_accuracy'], lr_test_metrics['test_accuracy']],\n",
    "})\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"MODEL COMPARISON\")\n",
    "print(\"=\"*80)\n",
    "print(comparison_df.to_string(index=False))\n",
    "print(\"=\"*80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6cc6f883",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "MLFLOW RUNS\n",
      "================================================================================\n",
      "lr_baseline                    | Val Acc: 0.7648\n",
      "gbt_baseline                   | Val Acc: 0.7128\n",
      "rf_baseline                    | Val Acc: 0.8336\n",
      "\n",
      "✅ Best model: rf_baseline (0.8336)\n",
      "================================================================================\n"
     ]
    }
   ],
   "source": [
    "mlflow_client = mlflow.tracking.MlflowClient()\n",
    "experiment = mlflow_client.get_experiment_by_name(\"nsf_models\")\n",
    "\n",
    "if experiment:\n",
    "    runs = mlflow_client.search_runs(experiment_ids=[experiment.experiment_id])\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"MLFLOW RUNS\")\n",
    "    print(\"=\"*80)\n",
    "    \n",
    "    for run in runs:\n",
    "        accuracy = run.data.metrics['valid_accuracy']\n",
    "        print(f\"{run.info.run_name:30s} | Val Acc: {accuracy:.4f}\")\n",
    "    \n",
    "    best_run = max(runs, key=lambda r: r.data.metrics['valid_accuracy'])\n",
    "    mlflow_client.set_tag(best_run.info.run_id, \"deployment\", \"production\")\n",
    "    \n",
    "    print(f\"\\n✅ Best model: {best_run.info.run_name} ({best_run.data.metrics['valid_accuracy']:.4f})\")\n",
    "    print(\"=\"*80)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
